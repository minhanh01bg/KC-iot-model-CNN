{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense,Conv1D,Conv2D,Flatten,BatchNormalization,MaxPooling1D,Dropout,Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,MaxPooling2D,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation,Add,Reshape,Permute,LeakyReLU,UpSampling2D,Conv2DTranspose,Concatenate\n",
    "from tensorflow.keras.layers import Lambda,InputSpec,Layer,Input,Add,ZeroPadding2D,UpSampling2D,MaxPooling2D,Conv2D,Bidirectional,LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import flask\n",
    "import json\n",
    "\n",
    "\n",
    "# def load_dataset():\n",
    "\n",
    "ddata = pd.read_pickle('./data/test_labels.pkl')\n",
    "ddata['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddata.info()\n",
    "ddata.drop(['Timestamp'],axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddata\n",
    "# df.drop(['fwd_header_length'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "# df.drop(['protocol'], axis=1, inplace=True)\n",
    "# df.drop(['Dst Port'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[int, float])\n",
    "df.describe(include=[object]).transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with duplicate values: ', df.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True, keep=False, ignore_index=True)\n",
    "\n",
    "print('Data size AFTER deleteting instances containing duplicate values: ', df.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()\n",
    "df.isnull().sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with missing values: ', df.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove missing values\n",
    "df.dropna(axis=0, inplace=True, how=\"any\")\n",
    "\n",
    "print('Data size AFTER deleteting instances containing missing values: ', df.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with infinite values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if all values are finite.\n",
    "np.all(np.isfinite(df.drop(['Label'], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values to NaN\n",
    "df.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Check which labels are related to infinte values\n",
    "df[(df['Flow Byts/s'].isnull()) & (df['Flow Pkts/s'].isnull())].Label.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with infinite values: ', df.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove infinte values\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print('Data size AFTER deleteting instances containing infinite values: ', df.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with features with quasi null std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = df.std(numeric_only=True)\n",
    "dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Features that meet the threshold\n",
    "constant_features = [column for column, std in dataset_std.iteritems() if std < 0.01]\n",
    "\n",
    "# Drop the constant features\n",
    "df.drop(labels=constant_features, axis=1, inplace=True)\n",
    "print(constant_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifying outliers with interquartile range\n",
    "filt = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "print(filt.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df[['Pkt Size Avg', 'Bwd Seg Size Avg']], orient=\"h\")\n",
    "\n",
    "#plt.title('Summary of some variables containing outliers', fontsize=18)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join('./images', 'outliers.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dtype of some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Flow Byts/s', 'Flow Pkts/s']] = df[['Flow Byts/s', 'Flow Pkts/s']].apply(pd.to_numeric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new feature `Port Category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conds = [\n",
    "#     (df['dst_port'] >= 1) & (df['dst_port'] < 1024),\n",
    "#     (df['dst_port'] >= 1024) & (df['dst_port'] < 49152),\n",
    "#     (df['dst_port'] >= 49152) & (df['dst_port'] <= 65535)\n",
    "# ]\n",
    "\n",
    "# choices = [\n",
    "#     \"1 - 1023\", \n",
    "#     \"1024 - 49151\",\n",
    "#     \"49152 - 65535\"\n",
    "# ]\n",
    "\n",
    "# df.insert(1, 'destination_port_category', np.select(conds, choices, default=\"0\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_corr = df.corr()\n",
    "dataset_corr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=1.0)\n",
    "ax = sns.heatmap(dataset_corr, annot=False)\n",
    "fig.savefig(os.path.join('./images', 'correlation matrix.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & Apply mask\n",
    "mask = np.triu(np.ones_like(dataset_corr, dtype=bool))\n",
    "tri_df = dataset_corr.mask(mask)\n",
    "\n",
    "# Find Features that meet the threshold\n",
    "correlated_features = [c for c in tri_df.columns if any(tri_df[c] > 0.98)]\n",
    "print(correlated_features)\n",
    "# Drop the highly correlated features\n",
    "df.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "print(df.info())\n",
    "train_dataset = df\n",
    "print(train_dataset['Label'].value_counts())\n",
    "# df.to_csv('./final_1/final_had_p.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port Usage Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# benign_ports = df.loc[df['Label'] == 'Benign', 'destination_port_category']\n",
    "# malicious_ports = df.loc[df['Label'] != 'Benign', 'destination_port_category']\n",
    "\n",
    "# # get rid of rows with specific value\n",
    "# benign_ports = benign_ports[benign_ports != \"0\"]\n",
    "# malicious_ports = malicious_ports[malicious_ports != \"0\"]\n",
    "\n",
    "# # sum each port category column\n",
    "# benign_ports = benign_ports.value_counts()\n",
    "# malicious_ports = malicious_ports.value_counts()\n",
    "\n",
    "# indexes = np.arange(3)\n",
    "# width = 0.4\n",
    "# rect1 = plt.bar(indexes, benign_ports.values, width, color=\"steelblue\", label=\"benign\")\n",
    "# rect2 = plt.bar(indexes + width, malicious_ports.values, width, color=\"indianred\", label=\"malicious\")\n",
    "\n",
    "# def add_text(rect):\n",
    "#     # add text to top of each bar\n",
    "#     for r in rect:\n",
    "#         h = r.get_height()\n",
    "#         plt.text(r.get_x() + r.get_width()/2, h*1.01, s=format(h, \",\") ,fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "# add_text(rect1)\n",
    "# add_text(rect2)\n",
    "\n",
    "# ax.set_xticks(indexes + width / 2)\n",
    "# ax.set_xticklabels([\"1 - 1,023\", \"1,024 - 49,151\", \"49,152 - 65,535\"])\n",
    "# plt.title('Distribution of Port Usage\\nAccording to Network Activity Type')\n",
    "# plt.xlabel('Port Range')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# fig.savefig(os.path.join('./images/', 'port_usage_comparison.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['Label'].value_counts()\n",
    "# encode label\n",
    "labelE = LabelEncoder()\n",
    "train_dataset['Label'] = labelE.fit_transform(train_dataset['Label'])\n",
    "train_dataset['Label'].value_counts()\n",
    "y = train_dataset['Label']\n",
    "train_dataset = train_dataset.drop(['Label'],axis=1)\n",
    "# train_dataset = train_dataset.drop(['destination_port_category'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['destination_port_category'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "# sc = StandardScaler()\n",
    "# train_dataset = sc.fit_transform(train_dataset)\n",
    "# train_dataset = pd.DataFrame(train_dataset,columns=df.columns[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataset, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling features to a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train_dataset.select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "numeric_features = train_dataset.select_dtypes(exclude=[object]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('categoricals', OneHotEncoder(drop='first', sparse=False, handle_unknown='error'), categorical_features),\n",
    "    ('numericals', QuantileTransformer(), numeric_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features)\n",
    "print(numeric_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features.tolist()\n",
    "X_train \n",
    "from pickle import dump\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train), columns=columns)\n",
    "dump(preprocessor, open('./normalization/preprocessor.pkl', 'wb'))\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test), columns=columns)\n",
    "X_val = pd.DataFrame(preprocessor.transform(X_val), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './final_1M/'\n",
    "X_train.to_pickle(os.path.join(DATA_DIR,  'train/train_features.pkl'))\n",
    "X_val.to_pickle(os.path.join(DATA_DIR,  'val/val_features.pkl'))\n",
    "X_test.to_pickle(os.path.join(DATA_DIR,  'test/test_features.pkl'))\n",
    "\n",
    "y_train.to_pickle(os.path.join(DATA_DIR,  'train/train_labels.pkl'))\n",
    "y_val.to_pickle(os.path.join(DATA_DIR,  'val/val_labels.pkl'))\n",
    "y_test.to_pickle(os.path.join(DATA_DIR,  'test/test_labels.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the training set using combination of SMOTE & RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def balance_dataset(X, y, undersampling_strategy, oversampling_strategy):\n",
    "\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=0)\n",
    "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
    "\n",
    "    over_sampler = SMOTE(sampling_strategy=oversampling_strategy)\n",
    "    X_bal, y_bal = over_sampler.fit_resample(X_under, y_under)\n",
    "    \n",
    "    return X_bal, y_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampling_strategy = {\n",
    "    0    :704017,\n",
    "    3    :600136,\n",
    "    1    :600098,\n",
    "    2    :593761\n",
    "}\n",
    "\n",
    "oversampling_strategy = {\n",
    "    0    :704017,\n",
    "    3    :600136,\n",
    "    1    :600098,\n",
    "    2    :593761\n",
    "}\n",
    "\n",
    "# Balance the training set\n",
    "X_train_bal, y_train_bal = balance_dataset(X_train, y_train, undersampling_strategy, oversampling_strategy)\n",
    "\n",
    "# Save the balanced training set\n",
    "X_train_bal.to_pickle(os.path.join(DATA_DIR,  'train/train_features_balanced.pkl'))\n",
    "y_train_bal.to_pickle(os.path.join(DATA_DIR,  'train/train_labels_balanced.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# sum each port category column\n",
    "imbalanced = y_train.value_counts()\n",
    "balanced = y_train_bal.value_counts()\n",
    "\n",
    "indexes = np.arange(4)\n",
    "width = 0.4\n",
    "rect1 = plt.bar(indexes, imbalanced.values, width, color=\"steelblue\", label=\"imbalanced\")\n",
    "rect2 = plt.bar(indexes + width, balanced.values, width, color=\"indianred\", label=\"balanced\")\n",
    "\n",
    "def add_text(rect):\n",
    "    \"\"\"Add text to top of each bar.\"\"\"\n",
    "    for r in rect:\n",
    "        h = r.get_height()\n",
    "        plt.text(r.get_x() + r.get_width()/2, h*1.01, s=format(h, \",\") ,fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "add_text(rect1)\n",
    "add_text(rect2)\n",
    "\n",
    "ax.set_xticks(indexes + width / 2)\n",
    "ax.set_xticklabels(['PortScan', 'DDoS',  'passwork_attack','Benign'])\n",
    "plt.xlabel('Traffic Activity', fontsize=16)\n",
    "plt.ylabel('# instances', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join('./images/', 'balanced_dataset.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl():\n",
    "    X_train = pd.read_pickle(os.path.join(DATA_DIR, 'train/train_features_balanced.pkl'))\n",
    "    y_train = pd.read_pickle(os.path.join(DATA_DIR, 'train/train_labels_balanced.pkl'))\n",
    "    X_val = pd.read_pickle(os.path.join(DATA_DIR, 'val/val_features.pkl'))\n",
    "    y_val = pd.read_pickle(os.path.join(DATA_DIR, 'val/val_labels.pkl'))\n",
    "    X_test = pd.read_pickle(os.path.join(DATA_DIR, 'test/test_features.pkl'))\n",
    "    y_test = pd.read_pickle(os.path.join(DATA_DIR, 'test/test_labels.pkl'))\n",
    "    return X_train,y_train,X_test,y_test,X_val,y_val\n",
    "\n",
    "def re_shape_2D(X_train,X_test,y_train,y_test,X_val,y_val):\n",
    "    X_train = X_train.to_numpy().reshape(len(X_train),X_train.shape[1]//8,8,1)\n",
    "    X_test = X_test.to_numpy().reshape(len(X_test),X_test.shape[1]//8,8,1)\n",
    "    X_val = X_val.to_numpy().reshape(len(X_val),X_val.shape[1]//8,8,1)\n",
    "    return X_train,X_test,y_train,y_test,X_val,y_val\n",
    "\n",
    "def re_shape_1D(X_train,X_test,y_train,y_test,X_val,y_val):\n",
    "    X_train = X_train.to_numpy().reshape(len(X_train),X_train.shape[1],1)\n",
    "    X_test = X_test.to_numpy().reshape(len(X_test),X_test.shape[1],1)\n",
    "    X_val = X_val.to_numpy().reshape(len(X_val),X_val.shape[1],1)\n",
    "    return X_train,X_test,y_train,y_test,X_val,y_val\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model_99(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    # load model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(Xtrain.shape[1],Xtrain.shape[2],Xtrain.shape[3]),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    # create data generator \n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    calls = [tensorboard_callback,\n",
    "            #  EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='auto'),\n",
    "            #  ModelCheckpoint(filepath=f'models/{time}/model_{time}.h5',monitor='val_loss',save_best_only=True,mode='auto'),\n",
    "            #  ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,mode='auto',min_delta=0.0001,cooldown=0,min_lr=0)\n",
    "            ]\n",
    "    model.fit(Xtrain,ytrain,epochs=epochs,batch_size=batch_size,validation_data=(Xtest,ytest),callbacks=calls,verbose=1)\n",
    "    return model\n",
    "\n",
    "def multi_head(time,Xtrain,ytrain,Xtest,ytest,Xval,yval,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    input1s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_1s1 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input1s)\n",
    "    batch_1s1 = BatchNormalization()(conv1d_1s1)\n",
    "    maxpool_1s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s1)\n",
    "    conv1d_1s2 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(maxpool_1s1)\n",
    "    batch_1s2 = BatchNormalization()(conv1d_1s2)\n",
    "    maxpool_1s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s2)\n",
    "    flat1 = Flatten()(maxpool_1s2)\n",
    "\n",
    "    input2s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_2s1 = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(input2s)\n",
    "    batch_2s1 = BatchNormalization()(conv1d_2s1)\n",
    "    maxpool_2s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s1)\n",
    "    conv1d_2s2 = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(maxpool_2s1)\n",
    "    batch_2s2 = BatchNormalization()(conv1d_2s2)\n",
    "    maxpool_2s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s2)\n",
    "    flat2 = Flatten()(maxpool_2s2)\n",
    "    \n",
    "    input3s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_3s1 = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(input3s)\n",
    "    batch_3s1 = BatchNormalization()(conv1d_3s1)\n",
    "    maxpool3 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s1)\n",
    "    conv1d_3s2 = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(maxpool3)\n",
    "    batch_3s2 = BatchNormalization()(conv1d_3s2)\n",
    "    maxpool_3s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s2)\n",
    "    flat3 = Flatten()(maxpool_3s2)\n",
    "\n",
    "    concat = concatenate([flat1,flat2,flat3])\n",
    "    dense1 = Dense(256,activation='relu')(concat)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=[input1s,input2s,input3s],outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    opt = SGD(lr=lr, momentum=0.9)  \n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    calls = [tensorboard_callback,\n",
    "             EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='auto'),\n",
    "             ModelCheckpoint(filepath=f'models/{time}/model_{time}.h5',monitor='val_loss',save_best_only=True,mode='auto'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,mode='auto',min_delta=0.0001,cooldown=0,min_lr=0)\n",
    "            ]\n",
    "    model.fit([Xtrain,Xtrain,Xtrain],ytrain,epochs=epochs,batch_size=batch_size,validation_data=([Xtest,Xtest,Xtest],ytest),callbacks=[tensorboard_callback],verbose=1)\n",
    "    # _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\n",
    "    # print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "    return model\n",
    "def multi_head1(time,Xtrain,ytrain,Xtest,ytest,Xval,yval,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    input1s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_1s1 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input1s)\n",
    "    batch_1s1 = BatchNormalization()(conv1d_1s1)\n",
    "    maxpool_1s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s1)\n",
    "    flat1 = Flatten()(maxpool_1s1)\n",
    "\n",
    "    input2s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_2s1 = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(input2s)\n",
    "    batch_2s1 = BatchNormalization()(conv1d_2s1)\n",
    "    maxpool_2s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s1)\n",
    "    flat2 = Flatten()(maxpool_2s1)\n",
    "    \n",
    "    input3s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_3s1 = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(input3s)\n",
    "    batch_3s1 = BatchNormalization()(conv1d_3s1)\n",
    "    maxpool3 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s1)\n",
    "    flat3 = Flatten()(maxpool3)\n",
    "\n",
    "    concat = concatenate([flat1,flat2,flat3])\n",
    "    dense1 = Dense(256,activation='relu')(concat)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=[input1s,input2s,input3s],outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    opt = SGD(lr=lr, momentum=0.9)  \n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    calls = [tensorboard_callback,\n",
    "             EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='auto'),\n",
    "             ModelCheckpoint(filepath=f'models/{time}/model_{time}.h5',monitor='val_loss',save_best_only=True,mode='auto'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,mode='auto',min_delta=0.0001,cooldown=0,min_lr=0)\n",
    "            ]\n",
    "    model.fit([Xtrain,Xtrain,Xtrain],ytrain,epochs=epochs,batch_size=batch_size,validation_data=([Xval,Xval,Xval],yval),callbacks=[tensorboard_callback],verbose=1)\n",
    "    _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "    return model\n",
    "\n",
    "def CNN_normal(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    # define model conv2D\n",
    "    input1s = Input(shape=(Xtrain.shape[1],Xtrain.shape[2],Xtrain.shape[3]))\n",
    "    conv1 = Conv2D(32,(3,3),activation='relu',padding='same')(input1s)\n",
    "    conv2 = Conv2D(32,(3,3),activation='relu',padding='same')(conv1)\n",
    "    bat = BatchNormalization()(conv2)\n",
    "    pool1 = MaxPooling2D((3,3))(bat)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    dense1 = Dense(256,activation='relu')(flat1)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=input1s,outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    opt = SGD(lr=lr, momentum=0.9)  \n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    calls = [tensorboard_callback,\n",
    "            EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto'),\n",
    "            ModelCheckpoint(filepath=f'./models/{time}/model_checkpoint{time}.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            #  ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,mode='auto',min_delta=0.0001,cooldown=0,min_lr=0)\n",
    "            ]\n",
    "    model.fit(Xtrain,ytrain,epochs=epochs,batch_size=batch_size,validation_data=(Xtest,ytest),callbacks=calls,verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test,X_val,y_val = load_pkl()\n",
    "X_train,X_test,y_train,y_test,X_val,y_val = re_shape_1D(X_train,X_test,y_train,y_test,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit\n",
    "time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "os.mkdir(f'./models/{time}')\n",
    "logdir=\"logs/multi/\" + time +'/'\n",
    "model = multi_head1(time,X_train,y_train,X_test,y_test,X_val,y_val,logdir,10,128)\n",
    "model.save(f'models/{time}/model_{time}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pd.DataFrame(y_test)\n",
    "da.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3ffae8020a817511732d4d0fe02060948831585d778b076daf6741c73f247a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
