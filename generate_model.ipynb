{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389115\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4098529 entries, 0 to 4499999\n",
      "Data columns (total 77 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Flow Duration      int64  \n",
      " 1   Tot Fwd Pkts       int64  \n",
      " 2   Tot Bwd Pkts       int64  \n",
      " 3   TotLen Fwd Pkts    float64\n",
      " 4   TotLen Bwd Pkts    float64\n",
      " 5   Fwd Pkt Len Max    float64\n",
      " 6   Fwd Pkt Len Min    float64\n",
      " 7   Fwd Pkt Len Mean   float64\n",
      " 8   Fwd Pkt Len Std    float64\n",
      " 9   Bwd Pkt Len Max    float64\n",
      " 10  Bwd Pkt Len Min    float64\n",
      " 11  Bwd Pkt Len Mean   float64\n",
      " 12  Bwd Pkt Len Std    float64\n",
      " 13  Flow Byts/s        float64\n",
      " 14  Flow Pkts/s        float64\n",
      " 15  Flow IAT Mean      float64\n",
      " 16  Flow IAT Std       float64\n",
      " 17  Flow IAT Max       float64\n",
      " 18  Flow IAT Min       float64\n",
      " 19  Fwd IAT Tot        float64\n",
      " 20  Fwd IAT Mean       float64\n",
      " 21  Fwd IAT Std        float64\n",
      " 22  Fwd IAT Max        float64\n",
      " 23  Fwd IAT Min        float64\n",
      " 24  Bwd IAT Tot        float64\n",
      " 25  Bwd IAT Mean       float64\n",
      " 26  Bwd IAT Std        float64\n",
      " 27  Bwd IAT Max        float64\n",
      " 28  Bwd IAT Min        float64\n",
      " 29  Fwd PSH Flags      int64  \n",
      " 30  Bwd PSH Flags      int64  \n",
      " 31  Fwd URG Flags      int64  \n",
      " 32  Bwd URG Flags      int64  \n",
      " 33  Fwd Header Len     int64  \n",
      " 34  Bwd Header Len     int64  \n",
      " 35  Fwd Pkts/s         float64\n",
      " 36  Bwd Pkts/s         float64\n",
      " 37  Pkt Len Min        float64\n",
      " 38  Pkt Len Max        float64\n",
      " 39  Pkt Len Mean       float64\n",
      " 40  Pkt Len Std        float64\n",
      " 41  Pkt Len Var        float64\n",
      " 42  FIN Flag Cnt       int64  \n",
      " 43  SYN Flag Cnt       int64  \n",
      " 44  RST Flag Cnt       int64  \n",
      " 45  PSH Flag Cnt       int64  \n",
      " 46  ACK Flag Cnt       int64  \n",
      " 47  URG Flag Cnt       int64  \n",
      " 48  CWE Flag Count     int64  \n",
      " 49  ECE Flag Cnt       int64  \n",
      " 50  Down/Up Ratio      float64\n",
      " 51  Pkt Size Avg       float64\n",
      " 52  Fwd Seg Size Avg   float64\n",
      " 53  Bwd Seg Size Avg   float64\n",
      " 54  Fwd Byts/b Avg     int64  \n",
      " 55  Fwd Pkts/b Avg     int64  \n",
      " 56  Fwd Blk Rate Avg   int64  \n",
      " 57  Bwd Byts/b Avg     int64  \n",
      " 58  Bwd Pkts/b Avg     int64  \n",
      " 59  Bwd Blk Rate Avg   int64  \n",
      " 60  Subflow Fwd Pkts   int64  \n",
      " 61  Subflow Fwd Byts   int64  \n",
      " 62  Subflow Bwd Pkts   int64  \n",
      " 63  Subflow Bwd Byts   int64  \n",
      " 64  Init Fwd Win Byts  int64  \n",
      " 65  Init Bwd Win Byts  int64  \n",
      " 66  Fwd Act Data Pkts  int64  \n",
      " 67  Fwd Seg Size Min   int64  \n",
      " 68  Active Mean        float64\n",
      " 69  Active Std         float64\n",
      " 70  Active Max         float64\n",
      " 71  Active Min         float64\n",
      " 72  Idle Mean          float64\n",
      " 73  Idle Std           float64\n",
      " 74  Idle Max           float64\n",
      " 75  Idle Min           float64\n",
      " 76  Label              object \n",
      "dtypes: float64(45), int64(31), object(1)\n",
      "memory usage: 2.4+ GB\n",
      "None\n",
      "Benign             1110538\n",
      "Port_scan          1000000\n",
      "DDOS                999990\n",
      "Password_attack     988001\n",
      "Name: Label, dtype: int64\n",
      "Benign             1110538\n",
      "Port_scan          1000000\n",
      "DDOS                999990\n",
      "Password_attack     988001\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1396/28818 [>.............................] - ETA: 8:16 - loss: 0.3579 - accuracy: 0.7542"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 264\u001b[0m\n\u001b[0;32m    262\u001b[0m os\u001b[39m.\u001b[39mmkdir(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./models/\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m logdir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogs/multi/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m time \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 264\u001b[0m model \u001b[39m=\u001b[39m mutil_head_double_layers_evaluate_model(time,X_train,y_train,X_test,y_test,logdir,\u001b[39m10\u001b[39;49m,\u001b[39m128\u001b[39;49m)\n\u001b[0;32m    265\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m}\u001b[39;00m\u001b[39m/model_\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    266\u001b[0m \u001b[39m# run_experiment(n_filters,X_train,X_test,y_train,y_test,1)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [1], line 162\u001b[0m, in \u001b[0;36mmutil_head_double_layers_evaluate_model\u001b[1;34m(time, Xtrain, ytrain, Xtest, ytest, logdir, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m    156\u001b[0m tensorboard_callback \u001b[39m=\u001b[39m TensorBoard(log_dir\u001b[39m=\u001b[39mlogdir)\n\u001b[0;32m    157\u001b[0m calls \u001b[39m=\u001b[39m [tensorboard_callback,\n\u001b[0;32m    158\u001b[0m          EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    159\u001b[0m          ModelCheckpoint(filepath\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m}\u001b[39;00m\u001b[39m/model_\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m,monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m    160\u001b[0m          ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,min_delta\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,cooldown\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,min_lr\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    161\u001b[0m         ]\n\u001b[1;32m--> 162\u001b[0m model\u001b[39m.\u001b[39;49mfit([Xtrain,Xtrain,Xtrain],ytrain,epochs\u001b[39m=\u001b[39;49mepochs,batch_size\u001b[39m=\u001b[39;49mbatch_size,validation_data\u001b[39m=\u001b[39;49m([Xtest,Xtest,Xtest],ytest),callbacks\u001b[39m=\u001b[39;49m[tensorboard_callback],verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m \u001b[39m# _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m# print(\"Accuracy: %.2f%%\" % (acc*100))\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\fancyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense,Conv1D,Conv2D,Flatten,BatchNormalization,MaxPooling1D,Dropout,Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,MaxPooling2D,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import flask\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    df = pd.read_csv('./final_1/final_1M.csv')\n",
    "    # df = df[df['Label'] != 'DDOS']\n",
    "    \n",
    "    # drop columns\n",
    "    df.drop(['Timestamp'], axis=1, inplace=True)\n",
    "    df.drop(['Protocol','Dst Port'],axis=1,inplace=True)\n",
    "    df.drop(['Flow Byts/s','Flow Pkts/s','Flow IAT Mean','Flow IAT Std','Flow IAT Max','Flow IAT Min'],axis=1,inplace=True)\n",
    "    # duplicated\n",
    "    print(df.duplicated().sum())\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # drop inf\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # drop columns\n",
    "    print(df.info())\n",
    "\n",
    "    print(df['Label'].value_counts())\n",
    "\n",
    "    train_dataset = df\n",
    "    print(train_dataset['Label'].value_counts())\n",
    "\n",
    "    # df.to_csv('./final_1/final_had_p.csv',index=False)\n",
    "\n",
    "    # encode label\n",
    "    labelE = LabelEncoder()\n",
    "    train_dataset['Label'] = labelE.fit_transform(train_dataset['Label'])\n",
    "    train_dataset['Label'].value_counts()\n",
    "    y = train_dataset['Label']\n",
    "    train_dataset = train_dataset.drop(['Label'],axis=1)\n",
    "\n",
    "    # normalize data\n",
    "    sc = StandardScaler()\n",
    "    train_dataset = sc.fit_transform(train_dataset)\n",
    "    train_dataset = pd.DataFrame(train_dataset,columns=df.columns[:-1])\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_dataset, y, test_size=0.1, random_state=42)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    train_dataset = pd.DataFrame(train_dataset,columns=df.columns[:-1])\n",
    "    X_test = sc.transform(X_train)\n",
    "    train_dataset = pd.DataFrame(train_dataset,columns=df.columns[:-1])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def re_shape(X_train,X_test,y_train,y_test):\n",
    "    X_train = X_train.to_numpy().reshape(len(X_train),X_train.shape[1],1)\n",
    "    X_test = X_test.to_numpy().reshape(len(X_test),X_test.shape[1],1)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "def mutil_head_evaluate_model(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    input1s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input1s)\n",
    "    batch = BatchNormalization()(conv1d)\n",
    "    maxpool = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch)\n",
    "    flat1 = Flatten()(maxpool)\n",
    "\n",
    "    input2s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv2d = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(input2s)\n",
    "    batch2 = BatchNormalization()(conv2d)\n",
    "    maxpool2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch2)\n",
    "    flat2 = Flatten()(maxpool2)\n",
    "    \n",
    "    input3s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv3d = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(input3s)\n",
    "    batch3 = BatchNormalization()(conv3d)\n",
    "    maxpool3 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch3)\n",
    "    flat3 = Flatten()(maxpool3)\n",
    "\n",
    "    concat = concatenate([flat1,flat2,flat3])\n",
    "    dense1 = Dense(256,activation='relu')(concat)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=[input1s,input2s,input3s],outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    opt = SGD(lr=lr, momentum=0.9)  \n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    \n",
    "    model.fit([Xtrain,Xtrain,Xtrain],ytrain,epochs=epochs,batch_size=batch_size,validation_data=([Xtest,Xtest,Xtest],ytest),callbacks=[tensorboard_callback],verbose=1)\n",
    "    _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\n",
    "    return acc\n",
    "\n",
    "def mutil_head_double_layers_evaluate_model(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr =0.001):\n",
    "    input1s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_1s1 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input1s)\n",
    "    batch_1s1 = BatchNormalization()(conv1d_1s1)\n",
    "    maxpool_1s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s1)\n",
    "    conv1d_1s2 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(maxpool_1s1)\n",
    "    batch_1s2 = BatchNormalization()(conv1d_1s2)\n",
    "    maxpool_1s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s2)\n",
    "    flat1 = Flatten()(maxpool_1s2)\n",
    "\n",
    "    input2s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_2s1 = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(input2s)\n",
    "    batch_2s1 = BatchNormalization()(conv1d_2s1)\n",
    "    maxpool_2s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s1)\n",
    "    conv1d_2s2 = Conv1D(filters=64,kernel_size=5,activation='relu',padding='same')(maxpool_2s1)\n",
    "    batch_2s2 = BatchNormalization()(conv1d_2s2)\n",
    "    maxpool_2s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s2)\n",
    "    flat2 = Flatten()(maxpool_2s2)\n",
    "    \n",
    "    input3s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_3s1 = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(input3s)\n",
    "    batch_3s1 = BatchNormalization()(conv1d_3s1)\n",
    "    maxpool3 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s1)\n",
    "    conv1d_3s2 = Conv1D(filters=128,kernel_size=7,activation='relu',padding='same')(maxpool3)\n",
    "    batch_3s2 = BatchNormalization()(conv1d_3s2)\n",
    "    maxpool_3s2 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s2)\n",
    "    flat3 = Flatten()(maxpool_3s2)\n",
    "\n",
    "    concat = concatenate([flat1,flat2,flat3])\n",
    "    dense1 = Dense(256,activation='relu')(concat)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=[input1s,input2s,input3s],outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    opt = SGD(lr=lr, momentum=0.9)  \n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    calls = [tensorboard_callback,\n",
    "             EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='auto'),\n",
    "             ModelCheckpoint(filepath=f'models/{time}/model_{time}.h5',monitor='val_loss',save_best_only=True,mode='auto'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,mode='auto',min_delta=0.0001,cooldown=0,min_lr=0)\n",
    "            ]\n",
    "    model.fit([Xtrain,Xtrain,Xtrain],ytrain,epochs=epochs,batch_size=batch_size,validation_data=([Xtest,Xtest,Xtest],ytest),callbacks=[tensorboard_callback],verbose=1)\n",
    "    # _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\n",
    "    # print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "    return model\n",
    "\n",
    "def multil_head_replace_unit(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr=0.0001):\n",
    "    input1s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_1s1 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input1s)\n",
    "    batch_1s1 = BatchNormalization()(conv1d_1s1)\n",
    "    maxpool_1s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_1s1)\n",
    "    flat1 = Flatten()(maxpool_1s1)\n",
    "\n",
    "    input2s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_2s1 = Conv1D(filters=32,kernel_size=5,activation='relu',padding='same')(input2s)\n",
    "    batch_2s1 = BatchNormalization()(conv1d_2s1)\n",
    "    maxpool_2s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_2s1)\n",
    "    flat2 = Flatten()(maxpool_2s1)\n",
    "\n",
    "    input3s = Input(shape=(Xtrain.shape[1],1))\n",
    "    conv1d_3s1 = Conv1D(filters=32,kernel_size=3,activation='relu',padding='same')(input3s)\n",
    "    batch_3s1 = BatchNormalization()(conv1d_3s1)\n",
    "    maxpool_3s1 = MaxPooling1D(pool_size=3,strides=2,padding='same')(batch_3s1)\n",
    "    flat3 = Flatten()(maxpool_3s1)\n",
    "\n",
    "    concat = concatenate([flat1,flat2,flat3])\n",
    "    dense1 = Dense(256,activation='relu')(concat)\n",
    "    drop = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(4,activation='softmax')(drop)\n",
    "    model = Model(inputs=[input1s,input2s,input3s],outputs=dense2)\n",
    "    #plot model\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "    opt = SGD(lr=lr, momentum=0.9)\n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    model.fit([Xtrain,Xtrain,Xtrain],ytrain,epochs=epochs,batch_size=batch_size,validation_data=([Xtest,Xtest,Xtest],ytest),callbacks=[tensorboard_callback],verbose=1)\n",
    "    _,acc = model.evaluate([Xtest,Xtest,Xtest],ytest,verbose=0)\n",
    "    return acc\n",
    "\n",
    "def model_CNN1(time,Xtrain,ytrain,Xtest,ytest,logdir,epochs=10,batch_size=128,lr = 0.0001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32,kernel_size=3,activation='relu',input_shape=Xtrain.shape[1:],padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=2,padding='same'))\n",
    "\n",
    "    model.add(Conv1D(filters=64,kernel_size=3,activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=2,padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "    plot_model(model, to_file=f'models/{time}/model_{time}.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    opt = SGD(lr=lr, momentum=0.9)\n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    logdir = logdir + time\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "    model.fit(Xtrain,ytrain,validation_data=(Xtest,ytest),epochs=epochs,batch_size=batch_size,callbacks=[tensorboard_callback],verbose=1)\n",
    "    _, accuracy = model.evaluate(Xtest, ytest, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def summarize_result(scores,n_filters,time):\n",
    "    print(scores,n_filters)\n",
    "    for fil in range(len(n_filters)):\n",
    "        m,s = np.mean(scores[fil]), np.std(scores[fil])\n",
    "        print('Batch_size #%d: %.3f%% (+/-%.3f)' % (n_filters[fil],m,s))\n",
    "    plt.boxplot(scores,labels=n_filters)\n",
    "    plt.savefig(f'models/{time}/exp_cnn_{time}.png')\n",
    "    plt.close()\n",
    "        \n",
    "def run_experiment(n_filters,X_train,X_test,y_train,y_test,repeats=10):\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "    os.mkdir(f'./models/{time}')\n",
    "    # repeat experiment\n",
    "    logdir=\"logs/multi/\" + time +'/'\n",
    "    all_scores = list()\n",
    "    for fil in n_filters:\n",
    "        scores = list()\n",
    "        for r in range(repeats):\n",
    "            score = mutil_head_double_layers_evaluate_model(time,X_train,y_train,X_test,y_test,logdir,10,fil)\n",
    "            score = score * 100.0\n",
    "            print('>#%d #%d: %.3f' % (fil,r+1,score))\n",
    "            scores.append(score)\n",
    "        all_scores.append(scores)\n",
    "    # summarize results\n",
    "    summarize_result(all_scores,n_filters,time)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_filters = [64,128,256]\n",
    "    X_train,X_test,y_train,y_test = load_dataset()\n",
    "    X_train,X_test,y_train,y_test = re_shape(X_train,X_test,y_train,y_test)\n",
    "    # n_filters = [256]\n",
    "    # %tensorboard --logdir logs/fit\n",
    "    time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    os.mkdir(f'./models/{time}')\n",
    "    logdir=\"logs/multi/\" + time +'/'\n",
    "    model = mutil_head_double_layers_evaluate_model(time,X_train,y_train,X_test,y_test,logdir,10,128)\n",
    "    model.save(f'models/{time}/model_{time}.h5')\n",
    "    # run_experiment(n_filters,X_train,X_test,y_train,y_test,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3ffae8020a817511732d4d0fe02060948831585d778b076daf6741c73f247a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
